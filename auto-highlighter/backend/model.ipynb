{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path, json_path):\n",
    "    \"\"\"\n",
    "    Load data from CSV and JSON files.\n",
    "    \"\"\"\n",
    "    # Load CSV\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading CSV file: {e}\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    if 'id' not in df.columns or 'result' not in df.columns:\n",
    "        raise ValueError(\"CSV must contain 'id' and 'result' columns.\")\n",
    "    \n",
    "    # Load JSON\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            highlights = json.load(f)\n",
    "    except Exception as e:\n",
    "        raise IOError(f\"Error reading JSON file: {e}\")\n",
    "    \n",
    "    return df, highlights\n",
    "\n",
    "def preprocess_labels(df, highlights):\n",
    "    \"\"\"\n",
    "    Assign binary labels to tokens based on highlights.json.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for id, highlighed_indices in highlights.items():\n",
    "        # Get the 'result' for the current id\n",
    "        row = df[df['id'] == id]\n",
    "        if row.empty:\n",
    "            print(f\"Warning: ID {id} not found in CSV.\")\n",
    "            continue\n",
    "        result_str = row['result'].values[0]\n",
    "        tokens = result_str.split()\n",
    "        labels = [1 if idx in highlighed_indices else 0 for idx in range(len(tokens))]\n",
    "        processed_data.append({\n",
    "            'id': id,\n",
    "            'tokens': tokens,\n",
    "            'labels': labels\n",
    "        })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def print_sample_tokens(processed_data, num_samples=2):\n",
    "    \"\"\"\n",
    "    Print sample tokens with their labels for verification.\n",
    "    \"\"\"\n",
    "    for sample in processed_data[:num_samples]:\n",
    "        print(f\"ID: {sample['id']}\")\n",
    "        print(\"Tokens and Labels:\")\n",
    "        for token, label in zip(sample['tokens'], sample['labels']):\n",
    "            print(f\"{token}: {label}\")\n",
    "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "def create_huggingface_dataset(processed_data):\n",
    "    \"\"\"\n",
    "    Convert processed data into a HuggingFace Dataset.\n",
    "    \"\"\"\n",
    "    dataset_dict = {\n",
    "        'id': [item['id'] for item in processed_data],\n",
    "        'tokens': [item['tokens'] for item in processed_data],\n",
    "        'labels': [item['labels'] for item in processed_data]\n",
    "    }\n",
    "    \n",
    "    dataset = Dataset.from_dict(dataset_dict)\n",
    "    return dataset\n",
    "\n",
    "# Tokenizer and model will be loaded globally for use in functions\n",
    "tokenizer = None  # Will be initialized in main\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokenize the inputs and align labels with tokenized outputs.\n",
    "    Assign labels 0 or 1 to all tokens, including subwords.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['tokens'],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding='max_length',\n",
    "        max_length=514,  # Adjust as needed\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['labels']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)  # Special tokens\n",
    "            else:\n",
    "                label_ids.append(label[word_id])\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs['labels'] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1-score for binary classification.\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Flatten the predictions and labels\n",
    "    true_labels = labels.flatten()\n",
    "    true_predictions = predictions.flatten()\n",
    "    \n",
    "    # Remove ignored index (label == -100)\n",
    "    mask = true_labels != -100\n",
    "    true_labels = true_labels[mask]\n",
    "    true_predictions = true_predictions[mask]\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, true_predictions, average='binary', zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = 'data.csv'         # Path to your data.csv\n",
    "JSON_PATH = 'highlights.json' # Path to your highlights.json\n",
    "\n",
    "# Step 1: Load data\n",
    "df, highlights = load_data(CSV_PATH, JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efded79845244f2a744ca3e7ef30ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d618476bf3b47a1bf606d013fe0998e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Preprocess labels\n",
    "processed_data = preprocess_labels(df, highlights)\n",
    "\n",
    "# Step 3: Print sample tokens for verification\n",
    "# print_sample_tokens(processed_data, num_samples=2)\n",
    "\n",
    "# Step 4: Create HuggingFace Dataset\n",
    "dataset = create_huggingface_dataset(processed_data)\n",
    "\n",
    "# Step 5: Split the dataset into train and validation\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "global tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", use_fast=True, add_prefix_space=True)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "514\n",
      "Tokens: [14826, 7, 3800, 3743, 1133, 7852, 2617, 13612, 52, 17, 27, 548, 57, 2754, 5, 539, 13, 81, 1718, 107, 4]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenized_train['input_ids'][0]\n",
    "selected = [tokens[i] for i in range(len(tokens)) if tokenized_train['labels'][0][i] == 1]\n",
    "print(len(tokenized_train['labels'][0]))\n",
    "len(tokenized_train['labels'][0])\n",
    "print(f\"Tokens: {selected}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Welcome to Carefree Janitorial Supply we’ve been serving the industry for over 35 years.\n"
     ]
    }
   ],
   "source": [
    "# Decode the tokens to get the original words\n",
    "decoded_words = tokenizer.decode(selected, skip_special_tokens=True)\n",
    "print(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Download configuration from huggingface.co and cache.\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze parameters in the classification head\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForTokenClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [label for label, pred in zip(label_seq, pred_seq) if label != -100]\n",
    "        for label_seq, pred_seq in zip(labels, predictions)\n",
    "    ]\n",
    "    true_preds = [\n",
    "        [pred for label, pred in zip(label_seq, pred_seq) if label != -100]\n",
    "        for label_seq, pred_seq in zip(labels, predictions)\n",
    "    ]\n",
    "\n",
    "    flat_true = [label for sublist in true_labels for label in sublist]\n",
    "    flat_preds = [pred for sublist in true_preds for pred in sublist]\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(flat_true, flat_preds, average='binary')\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 7. Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta_token_classifier\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# 8. Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60736a6995194c698e542b5852952abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c885c63c5c047afb1a583d774d47b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6119183301925659, 'eval_precision': 0.14655172413793102, 'eval_recall': 0.38636363636363635, 'eval_f1': 0.2125, 'eval_runtime': 0.0482, 'eval_samples_per_second': 20.731, 'eval_steps_per_second': 20.731, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5e86778b01436d842992608193f5ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6113185882568359, 'eval_precision': 0.1391304347826087, 'eval_recall': 0.36363636363636365, 'eval_f1': 0.20125786163522014, 'eval_runtime': 0.0505, 'eval_samples_per_second': 19.804, 'eval_steps_per_second': 19.804, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46acae6e39cc43a189d6e1851dff1806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6107853651046753, 'eval_precision': 0.1391304347826087, 'eval_recall': 0.36363636363636365, 'eval_f1': 0.20125786163522014, 'eval_runtime': 0.0482, 'eval_samples_per_second': 20.727, 'eval_steps_per_second': 20.727, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf12a43470c4f81ad74c196f83e16cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6103192567825317, 'eval_precision': 0.13274336283185842, 'eval_recall': 0.3409090909090909, 'eval_f1': 0.19108280254777069, 'eval_runtime': 0.0498, 'eval_samples_per_second': 20.089, 'eval_steps_per_second': 20.089, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ddf06781134659a4fc374bf6b87574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6099191308021545, 'eval_precision': 0.13274336283185842, 'eval_recall': 0.3409090909090909, 'eval_f1': 0.19108280254777069, 'eval_runtime': 0.0483, 'eval_samples_per_second': 20.724, 'eval_steps_per_second': 20.724, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e260e55775d49a0b010d53d7fee558e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.609585165977478, 'eval_precision': 0.13274336283185842, 'eval_recall': 0.3409090909090909, 'eval_f1': 0.19108280254777069, 'eval_runtime': 0.0487, 'eval_samples_per_second': 20.527, 'eval_steps_per_second': 20.527, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de6cf9f6d0c4de689dd06ad0772e920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6093194484710693, 'eval_precision': 0.13392857142857142, 'eval_recall': 0.3409090909090909, 'eval_f1': 0.1923076923076923, 'eval_runtime': 0.0547, 'eval_samples_per_second': 18.298, 'eval_steps_per_second': 18.298, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b8f4401ceb46afac26ec463c3007f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6091198325157166, 'eval_precision': 0.13636363636363635, 'eval_recall': 0.3409090909090909, 'eval_f1': 0.1948051948051948, 'eval_runtime': 0.0608, 'eval_samples_per_second': 16.438, 'eval_steps_per_second': 16.438, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df1541c3a8f4c8a93069ecf40a0e9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6089869737625122, 'eval_precision': 0.13636363636363635, 'eval_recall': 0.3409090909090909, 'eval_f1': 0.1948051948051948, 'eval_runtime': 0.0525, 'eval_samples_per_second': 19.041, 'eval_steps_per_second': 19.041, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8299314243bc46daa08ee6a0cc6222da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6089204549789429, 'eval_precision': 0.13636363636363635, 'eval_recall': 0.3409090909090909, 'eval_f1': 0.1948051948051948, 'eval_runtime': 0.0604, 'eval_samples_per_second': 16.562, 'eval_steps_per_second': 16.562, 'epoch': 10.0}\n",
      "{'train_runtime': 1.7102, 'train_samples_per_second': 5.847, 'train_steps_per_second': 5.847, 'train_loss': 0.6529894828796386, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d1267147a14607ab306517f5062873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.6089204549789429, 'eval_precision': 0.13636363636363635, 'eval_recall': 0.3409090909090909, 'eval_f1': 0.1948051948051948, 'eval_runtime': 0.051, 'eval_samples_per_second': 19.617, 'eval_steps_per_second': 19.617, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "    \n",
    "# Step 12: Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_highlighted_tokens(text, tokenizer, model, max_length=514, device=None):\n",
    "    \"\"\"\n",
    "    Inference function to predict highlighted tokens in a given text sequence.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text with space-separated tokens.\n",
    "        tokenizer: HuggingFace tokenizer.\n",
    "        model: Trained HuggingFace model.\n",
    "        max_length (int, optional): Maximum token length for the tokenizer. Defaults to 128.\n",
    "        device (str, optional): Device to run the model on ('cpu' or 'cuda').\n",
    "                                If None, automatically selects based on availability.\n",
    "\n",
    "    Prints:\n",
    "        - Original input text.\n",
    "        - List of tokens predicted as highlighted.\n",
    "    \"\"\"\n",
    "    # Determine the device to use\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Preprocess the input text\n",
    "    tokens = text.split()  # Assuming space-separated tokens\n",
    "    encoding = tokenizer(\n",
    "        tokens,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # Shape: (batch_size, seq_length, num_labels)\n",
    "    \n",
    "    predictions = torch.argmax(logits, dim=2).squeeze().tolist()  # Shape: (seq_length,)\n",
    "    \n",
    "    # Convert input_ids back to tokens\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    # Extract highlighted tokens (labels == 1), skipping special tokens and punctuation\n",
    "    highlighted_tokens = []\n",
    "    for token, pred in zip(predicted_tokens, predictions):\n",
    "        if token in tokenizer.all_special_tokens:\n",
    "            continue  # Skip special tokens like <s>, </s>, <pad>\n",
    "        # If you want to skip punctuation, uncomment the following lines:\n",
    "        # if token in punctuation_marks:\n",
    "        #     continue  # Skip punctuation tokens\n",
    "        if pred == 1:\n",
    "            # Clean the token by removing any leading 'Ġ' or other special characters if present\n",
    "            clean_token = token.replace('Ġ', '').replace('▁', '')\n",
    "            highlighted_tokens.append(clean_token)\n",
    "    \n",
    "    # Reconstruct the original sequence for display\n",
    "    original_sequence = ' '.join(tokens)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Original Sequence:\")\n",
    "    print(original_sequence)\n",
    "    \n",
    "    print(\"\\nPredicted Highlighted Tokens:\")\n",
    "    if highlighted_tokens:\n",
    "        print(' '.join(highlighted_tokens))\n",
    "    else:\n",
    "        print(\"No tokens predicted as highlighted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sequence:\n",
      "** Cleaning Supplies Trash Liners Cleaning Chemicals Equipment Skin Care Paper Products Janitorial Supplies and Equipment Welcome to Carefree Janitorial Supply we’ve been serving the industry for over 35 years. Our trucks service Northwest Louisiana and East Texas, Carefree Janitorial Supply prides itself in providing Quality Products, the Best Service, and the Most Competitive Prices. We are conveniently located just off I-20 at 405 Barksdale Boulevard in Bossier City, Louisiana. Please feel free to browse our online store. If you have any questions or comments contact us 24/7 at wecare@carefreejanitorial.com or for immediate assistance Monday-Friday 8 a.m. to 5 p.m.\n",
      "\n",
      "Predicted Highlighted Tokens:\n",
      "Trash ing Skin Paper Jan itor ial free Jan itor ial Supply ve serving over 35 . Our service and East free Jan itor ial Supply itself providing Quality the Best Service Prices We located off 405 ks dale Boulevard Boss Please free browse If have questions or comments 7 @ free jan itor ial assistance Monday Friday 5\n"
     ]
    }
   ],
   "source": [
    "infer_highlighted_tokens(' '.join(train_dataset['tokens'][0][:100]), tokenizer, model, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
